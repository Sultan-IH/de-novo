{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [21:33:59] Enabling RDKit 2019.09.1 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from statistics import stdev, mean\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CPU = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(thing, path):\n",
    "    with open(path, 'wb') as fp:\n",
    "        pkl.dump(thing, fp)\n",
    "    print(f'saved [{type(thing)}] to {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_lipinski.smi             druglike_lipinski_1k.smi\r\n",
      "all_lipinski.smi                druglike_lipinski_50k.test.smi\r\n",
      "\u001b[1m\u001b[36mby_num_atoms\u001b[m\u001b[m                    druglike_lipinski_50k.train.smi\r\n",
      "druglike_lipinski.smi           gcpn_smiles.500.smi\r\n",
      "druglike_lipinski_100k.smi      lstm_smiles.500.smi\r\n",
      "druglike_lipinski_10k.smi       very_active_lipinski.smi\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/lipinski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = Path('./data/lipinski/druglike_lipinski_100k.smi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_TOKEN = 'G'\n",
    "END_TOKEN = '\\n'\n",
    "PAD_TOKEN = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_SMILES = []\n",
    "with data_file.open() as fp:\n",
    "    for line in fp.readlines():\n",
    "        ALL_SMILES += [GO_TOKEN + line]\n",
    "NUM_TOTAL = len(ALL_SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SPLIT = 0.15\n",
    "split = int(NUM_TOTAL*TEST_SPLIT)\n",
    "TRAIN_SMILES = ALL_SMILES[split:]\n",
    "TEST_SMILES = ALL_SMILES[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(85000, 15000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TRAIN_SMILES), len(TEST_SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = list(set(''.join(ALL_SMILES+[PAD_TOKEN])))\n",
    "NUM_SYM = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYM_TO_ID = {s:i for i,s in enumerate(alphabet)}\n",
    "ID_TO_SYM = {i:s for s,i in SYM_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'l': 0,\n",
       " 'B': 1,\n",
       " '\\\\': 2,\n",
       " '6': 3,\n",
       " 'P': 4,\n",
       " 'O': 5,\n",
       " '[': 6,\n",
       " 'p': 7,\n",
       " '7': 8,\n",
       " '8': 9,\n",
       " '(': 10,\n",
       " '=': 11,\n",
       " '3': 12,\n",
       " '#': 13,\n",
       " '+': 14,\n",
       " 'n': 15,\n",
       " 's': 16,\n",
       " 'o': 17,\n",
       " 'C': 18,\n",
       " '/': 19,\n",
       " '@': 20,\n",
       " '5': 21,\n",
       " 'A': 22,\n",
       " '1': 23,\n",
       " 'H': 24,\n",
       " 'r': 25,\n",
       " '-': 26,\n",
       " '\\n': 27,\n",
       " 'I': 28,\n",
       " ']': 29,\n",
       " 'S': 30,\n",
       " ')': 31,\n",
       " 'F': 32,\n",
       " '4': 33,\n",
       " 'c': 34,\n",
       " 'N': 35,\n",
       " 'G': 36,\n",
       " '2': 37}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYM_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAX_ATOMS = max(map(lambda s: Chem.MolFromSmiles(s).GetNumAtoms(), D.keys()))\n",
    "MAX_SYM = max(map(len, ALL_SMILES)) # GO_TOKEN and END_TOKEN already considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(103, 100000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SYM, NUM_TOTAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(smiles):\n",
    "    \"\"\"\n",
    "    encode(simles): \n",
    "        - takes in a variable length smiles string (up to MAX_SYM) \n",
    "          and outputs a fixed size vector (MAX_SYM by NUM_SYM)\n",
    "        \n",
    "    \"\"\"\n",
    "    x = np.zeros((MAX_SYM, NUM_SYM))\n",
    "    x_n = len(smiles)\n",
    "    for i, sym in enumerate(smiles):\n",
    "        x[i, SYM_TO_ID[sym]] = 1\n",
    "    x[x_n:, SYM_TO_ID[PAD_TOKEN]] = 1\n",
    "    return x, x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    assert x.shape[1] == NUM_SYM\n",
    "    smiles = ''\n",
    "    for i in range(x.shape[0]):\n",
    "        topi = np.argmax(x[i, :])\n",
    "        smiles += ID_TO_SYM[topi]\n",
    "    return smiles\n",
    "def decode_short(x):\n",
    "    s = decode(x)\n",
    "    return  s[:s.find(END_TOKEN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCc1onc(NC(=O)c2ccc(Cl)cc2Cl)c1Br\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(ALL_SMILES[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCc1onc(NC(=O)c2ccc(Cl)cc2Cl)c1Br'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_short(encode(ALL_SMILES[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, smiles):\n",
    "        self.all_smiles = smiles\n",
    "        self.size = len(smiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        __getitem__(self, i):\n",
    "            - return \n",
    "        \"\"\"\n",
    "        x_i, x_n = encode(self.all_smiles[i])\n",
    "        \n",
    "        y_i = x_i[1:].copy()\n",
    "        x_i = x_i[:-1].copy()\n",
    "        \n",
    "        return x_i, y_i, x_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = SMILESDataset(TRAIN_SMILES)\n",
    "test_dataset = SMILESDataset(TEST_SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xi, yi, xlen = train_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GCC(C)Oc1ccc(cc1)c2cc(NCC(O)CO)c3ccccc3n2\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA',\n",
       " 'CC(C)Oc1ccc(cc1)c2cc(NCC(O)CO)c3ccccc3n2\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(xi), decode(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi, yi, xlen = test_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GFC(F)(F)Oc1cccc(NC(=O)c2oc(Br)cc2)c1\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA',\n",
       " 'FC(F)(F)Oc1cccc(NC(=O)c2oc(Br)cc2)c1\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(xi), decode(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 52"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, num_workers=NUM_CPU, shuffle=True)\n",
    "testloader = torch.utils.data.DataLoader(test_dataset, batch_size=len(TEST_SMILES), num_workers=NUM_CPU, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain LSTM model\n",
    "class GeneratorLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size, embedding_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            dropout=0.15)\n",
    "        \n",
    "        self.input_module = nn.Sequential(nn.Linear(input_size, 256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(0.10),\n",
    "                                         nn.Linear(256, 512),\n",
    "                                         nn.ReLU(),\n",
    "                                          nn.Dropout(0.10),\n",
    "                                         nn.Linear(512, embedding_size),\n",
    "                                         nn.ReLU())\n",
    "        \n",
    "        self.output_module = nn.Sequential(nn.Linear(hidden_size, 256),\n",
    "                                           nn.Dropout(0.10),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(256, output_size))\n",
    "\n",
    "        self.hidden = None \n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        return (Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)),\n",
    "                Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size)))\n",
    "\n",
    "    def forward(self, input_raw, pack=False, input_lens=None):\n",
    "        \"\"\"\n",
    "        forward(self, input_raw, state):\n",
    "             - input_raw = (bs, x_{i,t})\n",
    "             - state = (ht, ct)\n",
    "        \"\"\"\n",
    "        input_ = self.input_module(input_raw)\n",
    "        \n",
    "        if pack:\n",
    "            packed_input = pack_padded_sequence(input_, input_lens, batch_first=True)\n",
    "            input_, self.hidden = self.lstm(packed_input, self.hidden)\n",
    "            input_ = pad_packed_sequence(input_, batch_first=True)[0]\n",
    "            \n",
    "        input_ = self.output_module(input_)\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GeneratorLSTM(input_size = NUM_SYM, \n",
    "                      output_size = NUM_SYM, \n",
    "                      num_layers = 3, \n",
    "                      hidden_size = 512, \n",
    "                      embedding_size = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_optim = optim.Adam(list(model.parameters()), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorLSTM(\n",
       "  (lstm): LSTM(512, 512, num_layers=3, batch_first=True, dropout=0.15)\n",
       "  (input_module): Sequential(\n",
       "    (0): Linear(in_features=38, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (output_module): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=38, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has [6,849,062] trainable params\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'model has [{count_parameters(model):,}] trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, model_optim, dataloader, \n",
    "                    test_interval):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    loss_history = []\n",
    "    total = len(dataloader)\n",
    "    \n",
    "    test_loss_history = defaultdict(list)\n",
    "    new_smiles = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for batch_idx, batch in tqdm(enumerate(dataloader), total=total):\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "    \n",
    "        x_batch, y_batch, batch_lens = batch\n",
    "        \n",
    "        batch_size = x_batch.size(0)\n",
    "        max_len = int(max(batch_lens).item())\n",
    "        \n",
    "        x_batch = x_batch[:, 0:max_len, :]\n",
    "        y_batch = y_batch[:, 0:max_len, :]\n",
    "\n",
    "        # sort input\n",
    "        batch_len_sorted, sort_index = torch.sort(batch_lens, 0, descending=True)\n",
    "        batch_len_sorted = batch_len_sorted.numpy().tolist()\n",
    "        \n",
    "        x_batch = torch.index_select(x_batch, 0, sort_index)\n",
    "        y_batch = torch.index_select(y_batch, 0, sort_index)\n",
    "\n",
    "        x_batch = Variable(x_batch.float())\n",
    "        y_batch = Variable(y_batch.float())\n",
    "\n",
    "        # init state\n",
    "        model.hidden = model.init_hidden(batch_size=x_batch.size(0))\n",
    "        \n",
    "        y_pred = model(x_batch, pack=True, input_lens=batch_len_sorted)\n",
    "        \n",
    "        y_pred = F.log_softmax(y_pred.view(-1, NUM_SYM), dim=-1)\n",
    "        _, y_batch = y_batch.topk(1, dim=-1)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        \n",
    "        if batch_idx % test_interval == 0:\n",
    "            new_smiles[f'[batch num {batch_idx}] topk'] = generate(model)\n",
    "            new_smiles[f'[batch num {batch_idx}] softmax sample'] = generate(model, sample_f=softmax_temp_sample)\n",
    "\n",
    "            test_loss_history[batch_idx] = test(model, testloader)\n",
    "            print(f'[batch num: {batch_idx}] sampled')\n",
    "            \n",
    "            path = Path(f'./data/lipinski/results/lstm-1-epoch-{batch_idx}-batch-{loss.data.item():.4f}-loss')\n",
    "            path.mkdir(exist_ok=True)\n",
    "            \n",
    "            torch.save(model.state_dict(), str(path / 'model_dict.torch'))\n",
    "            save(new_smiles, str(path / 'generated_smiles.dict' ))\n",
    "            save(test_loss_history, str(path / 'test_loss.dict' ))\n",
    "\n",
    "        \n",
    "        print(f'[batch num: {batch_idx}] loss: {loss.data.item():.4f}')\n",
    "                \n",
    "    return loss_history, test_loss_history, new_smiles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_temp_sample(y_t, temperature = 1.0):\n",
    "    prediction_vector = F.softmax(y_t / temperature, dim=-1)\n",
    "    x_index_t = torch.multinomial(prediction_vector, 1)[:, 0]\n",
    "    return x_index_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sample(y_t):\n",
    "    _, pred_idx =  y_t.topk(1, dim=-1)\n",
    "    return pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, test_samples=5, sample_f=topk_sample, v=True):\n",
    "    model.eval()\n",
    "    \n",
    "    x = torch.zeros(test_samples, MAX_SYM, NUM_SYM)\n",
    "    \n",
    "    x[:, 0, SYM_TO_ID[GO_TOKEN]] = 1\n",
    "    \n",
    "    for i in range(MAX_SYM-1):\n",
    "        pred = model(x)\n",
    "        pred_idx = softmax_temp_sample(pred[:,i,:])\n",
    "        \n",
    "        temp = torch.zeros(test_samples, MAX_SYM, NUM_SYM)\n",
    "        temp[:, i+1, pred_idx] = 1  \n",
    "        x.add_(temp)\n",
    "    \n",
    "    if v: print('\\n',10*'-' + 'GENERATED SMILES STRINGS' + 10*'-')\n",
    "    smiles = []\n",
    "    \n",
    "    for j in range(test_samples):\n",
    "        s = decode_short(x[j].numpy())\n",
    "        smiles += [s]\n",
    "        \n",
    "        if v: print(s)\n",
    "            \n",
    "    return smiles\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    model.eval()\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in testloader:\n",
    "        \n",
    "        x_batch, y_batch, batch_lens = batch\n",
    "        \n",
    "        batch_size = x_batch.size(0)\n",
    "        max_len = int(max(batch_lens).item())\n",
    "        \n",
    "        x_batch = x_batch[:, 0:max_len, :]\n",
    "        y_batch = y_batch[:, 0:max_len, :]\n",
    "\n",
    "        # sort input\n",
    "        batch_len_sorted, sort_index = torch.sort(batch_lens, 0, descending=True)\n",
    "        batch_len_sorted = batch_len_sorted.numpy().tolist()\n",
    "        \n",
    "        x_batch = torch.index_select(x_batch, 0, sort_index)\n",
    "        y_batch = torch.index_select(y_batch, 0, sort_index)\n",
    "\n",
    "        x_batch = Variable(x_batch.float())\n",
    "        y_batch = Variable(y_batch.float())\n",
    "\n",
    "        # init state\n",
    "        model.hidden = model.init_hidden(batch_size=x_batch.size(0))\n",
    "        \n",
    "        y_pred = model(x_batch, pack=True, input_lens=batch_len_sorted)\n",
    "        \n",
    "        y_pred = y_pred.view(-1, NUM_SYM)\n",
    "        _, y_batch = y_batch.topk(1, dim=-1)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        \n",
    "        total_loss += criterion(y_pred, y_batch)\n",
    "                \n",
    "    return total_loss / len(test_dataset)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9677bd1113b4769989bd7be22ec66b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1635), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "G667\\pp/O38#s\\p\\OBOsB\\[sO\\pr#B\\\\p\\=n\\P6O=7OOOP8n[P\\6BBln(O6(l8l7O\\p#ppn[\\Ol\\\\Bln[OOlBBOp[PlB6B7O8B(BBO\n",
      "G667\\pp/O38#s\\p\\OBOsB\\[sO\\pr#B\\\\p\\=n\\P6O=7OOOP8n[P\\6BBln(O6(l8l7O\\p#ppn[\\Ol\\\\Bln[OOlBBOp[PlB6B7O8B(BBO\n",
      "G667\\pp/O38#s\\p\\OBOsB\\[sO\\pr#B\\\\p\\=n\\P6O=7OOOP8n[P\\6BBln(O6(l8l7O\\p#ppn[\\Ol\\\\Bln[OOlBBOp[PlB6B7O8B(BBO\n",
      "G667\\pp/O38#s\\p\\OBOsB\\[sO\\pr#B\\\\p\\=n\\P6O=7OOOP8n[P\\6BBln(O6(l8l7O\\p#ppn[\\Ol\\\\Bln[OOlBBOp[PlB6B7O8B(BBO\n",
      "G667\\pp/O38#s\\p\\OBOsB\\[sO\\pr#B\\\\p\\=n\\P6O=7OOOP8n[P\\6BBln(O6(l8l7O\\p#ppn[\\Ol\\\\Bln[OOlBBOp[PlB6B7O8B(BBO\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "G=OB((68lBP=n@l[B8=BPO\\BOC6[l7P=OOBP\\l\\PBlBl(@n\\l35s6BpB3Pl6lP6llP3=\\B[8\\67\\\\p88P7\\Bl6Pl666BB=B6lBP\\P/\n",
      "G=OB((68lBP=n@l[B8=BPO\\BOC6[l7P=OOBP\\l\\PBlBl(@n\\l35s6BpB3Pl6lP6llP3=\\B[8\\67\\\\p88P7\\Bl6Pl666BB=B6lBP\\P/\n",
      "G=OB((68lBP=n@l[B8=BPO\\BOC6[l7P=OOBP\\l\\PBlBl(@n\\l35s6BpB3Pl6lP6llP3=\\B[8\\67\\\\p88P7\\Bl6Pl666BB=B6lBP\\P/\n",
      "G=OB((68lBP=n@l[B8=BPO\\BOC6[l7P=OOBP\\l\\PBlBl(@n\\l35s6BpB3Pl6lP6llP3=\\B[8\\67\\\\p88P7\\Bl6Pl666BB=B6lBP\\P/\n",
      "G=OB((68lBP=n@l[B8=BPO\\BOC6[l7P=OOBP\\l\\PBlBl(@n\\l35s6BpB3Pl6lP6llP3=\\B[8\\67\\\\p88P7\\Bl6Pl666BB=B6lBP\\P/\n",
      "[batch num: 0] sampled\n",
      "saved [<class 'collections.defaultdict'>] to data/lipinski/results/lstm-1-epoch-0-batch-3.3965-loss/generated_smiles.dict\n",
      "saved [<class 'collections.defaultdict'>] to data/lipinski/results/lstm-1-epoch-0-batch-3.3965-loss/test_loss.dict\n",
      "[batch num: 0] loss: 3.3965\n",
      "[batch num: 1] loss: 4.1018\n",
      "[batch num: 2] loss: 3.0299\n",
      "[batch num: 3] loss: 3.0626\n",
      "[batch num: 4] loss: 3.1214\n",
      "[batch num: 5] loss: 3.0780\n",
      "[batch num: 6] loss: 3.0667\n",
      "[batch num: 7] loss: 2.8959\n",
      "[batch num: 8] loss: 2.8897\n",
      "[batch num: 9] loss: 2.8448\n",
      "[batch num: 10] loss: 2.7688\n",
      "[batch num: 11] loss: 2.8270\n",
      "[batch num: 12] loss: 2.8035\n",
      "[batch num: 13] loss: 2.8159\n",
      "[batch num: 14] loss: 2.8056\n",
      "[batch num: 15] loss: 2.7373\n",
      "[batch num: 16] loss: 2.7275\n",
      "[batch num: 17] loss: 2.6823\n",
      "[batch num: 18] loss: 2.6575\n",
      "[batch num: 19] loss: 2.6079\n",
      "[batch num: 20] loss: 2.6193\n",
      "[batch num: 21] loss: 2.5803\n",
      "[batch num: 22] loss: 2.5560\n",
      "[batch num: 23] loss: 2.5401\n",
      "[batch num: 24] loss: 2.4748\n",
      "[batch num: 25] loss: 2.4554\n",
      "[batch num: 26] loss: 2.5222\n",
      "[batch num: 27] loss: 2.4283\n",
      "[batch num: 28] loss: 2.3372\n",
      "[batch num: 29] loss: 2.3395\n",
      "[batch num: 30] loss: 2.3918\n",
      "[batch num: 31] loss: 2.2382\n",
      "[batch num: 32] loss: 2.2404\n",
      "[batch num: 33] loss: 2.0402\n",
      "[batch num: 34] loss: 2.0563\n",
      "[batch num: 35] loss: 2.0831\n",
      "[batch num: 36] loss: 2.0430\n",
      "[batch num: 37] loss: 2.0725\n",
      "[batch num: 38] loss: 2.0615\n",
      "[batch num: 39] loss: 1.8065\n",
      "[batch num: 40] loss: 1.9791\n",
      "[batch num: 41] loss: 1.9805\n",
      "[batch num: 42] loss: 1.6061\n",
      "[batch num: 43] loss: 1.7488\n",
      "[batch num: 44] loss: 1.8171\n",
      "[batch num: 45] loss: 1.5664\n",
      "[batch num: 46] loss: 1.8988\n",
      "[batch num: 47] loss: 1.7926\n",
      "[batch num: 48] loss: 1.5463\n",
      "[batch num: 49] loss: 1.7076\n",
      "[batch num: 50] loss: 1.6350\n",
      "[batch num: 51] loss: 2.1946\n",
      "[batch num: 52] loss: 1.3804\n",
      "[batch num: 53] loss: 1.5122\n",
      "[batch num: 54] loss: 1.7259\n",
      "[batch num: 55] loss: 1.7462\n",
      "[batch num: 56] loss: 1.7129\n",
      "[batch num: 57] loss: 1.8784\n",
      "[batch num: 58] loss: 1.5172\n",
      "[batch num: 59] loss: 1.7098\n",
      "[batch num: 60] loss: 1.9498\n",
      "[batch num: 61] loss: 1.3942\n",
      "[batch num: 62] loss: 1.7958\n",
      "[batch num: 63] loss: 1.7429\n",
      "[batch num: 64] loss: 1.8138\n",
      "[batch num: 65] loss: 1.3873\n",
      "[batch num: 66] loss: 1.7676\n",
      "[batch num: 67] loss: 1.4434\n",
      "[batch num: 68] loss: 1.2684\n",
      "[batch num: 69] loss: 1.4123\n",
      "[batch num: 70] loss: 1.5318\n",
      "[batch num: 71] loss: 1.3803\n",
      "[batch num: 72] loss: 1.3026\n",
      "[batch num: 73] loss: 1.7732\n",
      "[batch num: 74] loss: 1.3225\n",
      "[batch num: 75] loss: 1.4260\n",
      "[batch num: 76] loss: 1.3013\n",
      "[batch num: 77] loss: 1.2683\n",
      "[batch num: 78] loss: 1.6663\n",
      "[batch num: 79] loss: 1.3755\n",
      "[batch num: 80] loss: 1.4138\n",
      "[batch num: 81] loss: 1.5588\n",
      "[batch num: 82] loss: 1.7065\n",
      "[batch num: 83] loss: 1.1168\n",
      "[batch num: 84] loss: 1.8299\n",
      "[batch num: 85] loss: 1.2981\n",
      "[batch num: 86] loss: 1.6190\n",
      "[batch num: 87] loss: 1.7142\n",
      "[batch num: 88] loss: 1.6680\n",
      "[batch num: 89] loss: 1.3089\n",
      "[batch num: 90] loss: 1.4692\n",
      "[batch num: 91] loss: 1.3764\n",
      "[batch num: 92] loss: 1.5122\n",
      "[batch num: 93] loss: 1.3616\n",
      "[batch num: 94] loss: 1.4486\n",
      "[batch num: 95] loss: 1.2664\n",
      "[batch num: 96] loss: 1.5485\n",
      "[batch num: 97] loss: 1.2271\n",
      "[batch num: 98] loss: 1.4468\n",
      "[batch num: 99] loss: 1.5996\n",
      "[batch num: 100] loss: 1.4272\n",
      "[batch num: 101] loss: 1.4334\n",
      "[batch num: 102] loss: 1.1490\n",
      "[batch num: 103] loss: 1.6758\n",
      "[batch num: 104] loss: 1.3778\n",
      "[batch num: 105] loss: 1.2909\n",
      "[batch num: 106] loss: 1.5848\n",
      "[batch num: 107] loss: 1.3980\n",
      "[batch num: 108] loss: 1.1992\n",
      "[batch num: 109] loss: 1.5231\n",
      "[batch num: 110] loss: 1.3260\n",
      "[batch num: 111] loss: 1.1154\n",
      "[batch num: 112] loss: 1.5117\n",
      "[batch num: 113] loss: 1.3666\n",
      "[batch num: 114] loss: 1.2889\n",
      "[batch num: 115] loss: 1.5928\n",
      "[batch num: 116] loss: 1.2214\n",
      "[batch num: 117] loss: 1.3096\n",
      "[batch num: 118] loss: 1.3881\n",
      "[batch num: 119] loss: 1.4273\n",
      "[batch num: 120] loss: 1.3844\n",
      "[batch num: 121] loss: 1.4371\n",
      "[batch num: 122] loss: 1.3933\n",
      "[batch num: 123] loss: 1.1127\n",
      "[batch num: 124] loss: 1.3463\n",
      "[batch num: 125] loss: 1.1324\n",
      "[batch num: 126] loss: 1.3890\n",
      "[batch num: 127] loss: 1.5864\n",
      "[batch num: 128] loss: 1.3940\n",
      "[batch num: 129] loss: 1.0849\n",
      "[batch num: 130] loss: 1.1103\n",
      "[batch num: 131] loss: 1.1605\n",
      "[batch num: 132] loss: 1.3139\n",
      "[batch num: 133] loss: 1.3982\n",
      "[batch num: 134] loss: 1.1445\n",
      "[batch num: 135] loss: 1.1888\n",
      "[batch num: 136] loss: 0.9229\n",
      "[batch num: 137] loss: 0.9936\n",
      "[batch num: 138] loss: 1.2396\n",
      "[batch num: 139] loss: 1.3911\n",
      "[batch num: 140] loss: 1.0196\n",
      "[batch num: 141] loss: 1.2740\n",
      "[batch num: 142] loss: 1.1495\n",
      "[batch num: 143] loss: 0.9997\n",
      "[batch num: 144] loss: 1.3266\n",
      "[batch num: 145] loss: 0.9199\n",
      "[batch num: 146] loss: 0.8650\n",
      "[batch num: 147] loss: 1.1167\n",
      "[batch num: 148] loss: 1.4113\n",
      "[batch num: 149] loss: 1.0873\n",
      "[batch num: 150] loss: 0.9301\n",
      "[batch num: 151] loss: 1.1087\n",
      "[batch num: 152] loss: 0.8771\n",
      "[batch num: 153] loss: 1.0785\n",
      "[batch num: 154] loss: 1.0853\n",
      "[batch num: 155] loss: 1.2767\n",
      "[batch num: 156] loss: 0.9913\n",
      "[batch num: 157] loss: 1.3224\n",
      "[batch num: 158] loss: 1.1680\n",
      "[batch num: 159] loss: 0.9920\n",
      "[batch num: 160] loss: 1.2121\n",
      "[batch num: 161] loss: 1.0091\n",
      "[batch num: 162] loss: 0.9584\n",
      "[batch num: 163] loss: 1.0663\n",
      "[batch num: 164] loss: 1.0413\n",
      "[batch num: 165] loss: 1.1372\n",
      "[batch num: 166] loss: 0.9449\n",
      "[batch num: 167] loss: 1.1730\n",
      "[batch num: 168] loss: 1.1301\n",
      "[batch num: 169] loss: 1.1171\n",
      "[batch num: 170] loss: 1.2855\n",
      "[batch num: 171] loss: 1.1339\n",
      "[batch num: 172] loss: 1.2236\n",
      "[batch num: 173] loss: 1.1029\n",
      "[batch num: 174] loss: 0.9292\n",
      "[batch num: 175] loss: 0.9278\n",
      "[batch num: 176] loss: 0.9949\n",
      "[batch num: 177] loss: 0.8603\n",
      "[batch num: 178] loss: 1.1403\n",
      "[batch num: 179] loss: 0.8643\n",
      "[batch num: 180] loss: 1.1397\n",
      "[batch num: 181] loss: 0.9128\n",
      "[batch num: 182] loss: 1.1190\n",
      "[batch num: 183] loss: 0.8931\n",
      "[batch num: 184] loss: 0.9478\n",
      "[batch num: 185] loss: 1.2059\n",
      "[batch num: 186] loss: 1.1617\n",
      "[batch num: 187] loss: 1.0016\n",
      "[batch num: 188] loss: 1.0862\n",
      "[batch num: 189] loss: 0.8467\n",
      "[batch num: 190] loss: 1.1814\n",
      "[batch num: 191] loss: 1.1602\n",
      "[batch num: 192] loss: 1.0336\n",
      "[batch num: 193] loss: 1.0560\n",
      "[batch num: 194] loss: 1.1520\n",
      "[batch num: 195] loss: 0.8064\n",
      "[batch num: 196] loss: 1.1661\n",
      "[batch num: 197] loss: 1.0303\n",
      "[batch num: 198] loss: 0.8498\n",
      "[batch num: 199] loss: 1.0582\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn=AAAAAAAAA\n",
      "GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn=AAAAAAAAA\n",
      "GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn=AAAAAAAAA\n",
      "GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn=AAAAAAAAA\n",
      "GAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAn=AAAAAAAAA\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "GAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n",
      "GAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAACAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA\n"
     ]
    }
   ],
   "source": [
    "train_history, train_history, new_smiles = train_one_epoch(model, model_optim, dataloader, test_interval=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, l=-1):\n",
    "    if l < 0:\n",
    "        l = len(history)\n",
    "    fig = px.line(x=np.arange(l), y=history[:l], labels={'x':'batch number', 'y':'binary cross-entropy loss'})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (dn2d)",
   "language": "python",
   "name": "dn2d"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
