{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pad_packed_sequence, pack_padded_sequence\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import MultiStepLR, CosineAnnealingLR\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import networkx as nx\n",
    "\n",
    "from rdkit import Chem\n",
    "from rdkit import DataStructs\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import QED\n",
    "from rdkit import RDLogger   \n",
    "\n",
    "\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "from IPython.display import SVG\n",
    "\n",
    "from collections import defaultdict\n",
    "import multiprocessing as mp\n",
    "from statistics import stdev, mean\n",
    "from pathlib import Path\n",
    "import pickle as pkl\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CPU = 3\n",
    "SEED = 3287450"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "sns.set(style='white', context='notebook', rc={'figure.figsize':(14,10)})\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "RDLogger.DisableLog('rdApp.*') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(thing, path):\n",
    "    with open(path, 'wb') as fp:\n",
    "        pkl.dump(thing, fp)\n",
    "    print(f'saved [{type(thing)}] to {path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(path):\n",
    "    with open(path, 'rb') as fp:\n",
    "        thing = pkl.load(fp)\n",
    "    return thing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "active_lipinski.smi\t\t gcpn_smiles.500.smi\r\n",
      "all_lipinski.smi\t\t lstm_smiles.500.smi\r\n",
      "druglike_lipinski_100k.smi\t qed_df_full.pickle\r\n",
      "druglike_lipinski_10k.smi\t qed_df.pickle\r\n",
      "druglike_lipinski_1k.smi\t qed_df_small.activity.pickle\r\n",
      "druglike_lipinski_50k.test.smi\t results\r\n",
      "druglike_lipinski_50k.train.smi  very_active_lipinski.smi\r\n",
      "druglike_lipinski.smi\r\n"
     ]
    }
   ],
   "source": [
    "!ls ./data/lipinski"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "GO_TOKEN = 'G'\n",
    "END_TOKEN = '\\n'\n",
    "PAD_TOKEN = 'A'\n",
    "\n",
    "DATA_PATH = Path('./data/lipinski/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_smiles(path, test_split=0.15):\n",
    "    all_smiles = []\n",
    "    with open(path) as fp:\n",
    "        for line in tqdm(fp.readlines()):\n",
    "            all_smiles += [GO_TOKEN + line]\n",
    "    size = len(all_smiles)\n",
    "    split = int(size * test_split)\n",
    "    return all_smiles[split:] ,  all_smiles[:split]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80d5e902bde9410c91cd4c2bcd761182",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=622802.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46a6d86045a149a195d92b13b052aba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a5d830bb28445ac883b9d875b02258b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=482.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "P1_SMILES = load_smiles(DATA_PATH / 'druglike_lipinski.smi')\n",
    "P2_SMILES = load_smiles(DATA_PATH / 'active_lipinski.smi', test_split=0.1)\n",
    "P3_SMILES = load_smiles(DATA_PATH / 'very_active_lipinski.smi', test_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "624368"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ALL_SMILES = P1_SMILES[0] + P1_SMILES[1] + P2_SMILES[0] + P2_SMILES[1] + P3_SMILES[0] + P3_SMILES[1]\n",
    "len(ALL_SMILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = sorted(list(set(''.join(ALL_SMILES+[PAD_TOKEN]))))\n",
    "NUM_SYM = len(alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYM_TO_ID = {s:i for i,s in enumerate(alphabet)}\n",
    "ID_TO_SYM = {i:s for s,i in SYM_TO_ID.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n': 0,\n",
       " '#': 1,\n",
       " '%': 2,\n",
       " '(': 3,\n",
       " ')': 4,\n",
       " '+': 5,\n",
       " '-': 6,\n",
       " '/': 7,\n",
       " '0': 8,\n",
       " '1': 9,\n",
       " '2': 10,\n",
       " '3': 11,\n",
       " '4': 12,\n",
       " '5': 13,\n",
       " '6': 14,\n",
       " '7': 15,\n",
       " '8': 16,\n",
       " '9': 17,\n",
       " '=': 18,\n",
       " '@': 19,\n",
       " 'A': 20,\n",
       " 'B': 21,\n",
       " 'C': 22,\n",
       " 'F': 23,\n",
       " 'G': 24,\n",
       " 'H': 25,\n",
       " 'I': 26,\n",
       " 'N': 27,\n",
       " 'O': 28,\n",
       " 'P': 29,\n",
       " 'S': 30,\n",
       " '[': 31,\n",
       " '\\\\': 32,\n",
       " ']': 33,\n",
       " 'c': 34,\n",
       " 'l': 35,\n",
       " 'n': 36,\n",
       " 'o': 37,\n",
       " 'p': 38,\n",
       " 'r': 39,\n",
       " 's': 40}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SYM_TO_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# MAX_ATOMS = max(map(lambda s: Chem.MolFromSmiles(s).GetNumAtoms(), D.keys()))\n",
    "MAX_SYM = max(map(len, ALL_SMILES)) # GO_TOKEN and END_TOKEN already considered "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SYM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(smiles):\n",
    "    \"\"\"\n",
    "    encode(simles): \n",
    "        - takes in a variable length smiles string (up to MAX_SYM) \n",
    "          and outputs a fixed size vector (MAX_SYM by NUM_SYM)\n",
    "        \n",
    "    \"\"\"\n",
    "    x = np.zeros((MAX_SYM, NUM_SYM))\n",
    "    x_n = len(smiles)\n",
    "    for i, sym in enumerate(smiles):\n",
    "        x[i, SYM_TO_ID[sym]] = 1\n",
    "    x[x_n:, SYM_TO_ID[PAD_TOKEN]] = 1\n",
    "    return x, x_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(x):\n",
    "    assert x.shape[1] == NUM_SYM\n",
    "    smiles = ''\n",
    "    for i in range(x.shape[0]):\n",
    "        topi = np.argmax(x[i, :])\n",
    "        smiles += ID_TO_SYM[topi]\n",
    "    return smiles \n",
    "\n",
    "def decode_valid(x):\n",
    "    s = decode(x)\n",
    "    if s[0] == 'G':\n",
    "        s = s[1:]\n",
    "    return  s[:s.find(END_TOKEN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'GCn1c(SCC(=O)Nc2ccccc2)nnc1c3ccc4nonc4c3\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(encode(ALL_SMILES[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Cn1c(SCC(=O)Nc2ccccc2)nnc1c3ccc4nonc4c3'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_valid(encode(ALL_SMILES[0])[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMILESDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, smiles):\n",
    "        self.all_smiles = smiles\n",
    "        self.size = len(smiles)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        \"\"\"\n",
    "        __getitem__(self, i):\n",
    "            - return \n",
    "        \"\"\"\n",
    "        x_i, x_n = encode(self.all_smiles[i])\n",
    "        \n",
    "        y_i = x_i[1:].copy()\n",
    "        x_i = x_i[:-1].copy()\n",
    "        \n",
    "        return x_i, y_i, x_n\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_phase(data, pid, lr, interval, ):\n",
    "    Phase = namedtuple('Phase', ['pid','traindata', 'testdata', 'lr', 'test_interval'])\n",
    "    phase = Phase(traindata=SMILESDataset(data[0]),\n",
    "                  testdata=SMILESDataset(data[1]),\n",
    "                  lr=lr,\n",
    "                  pid=pid, # phase id\n",
    "                  test_interval=interval)\n",
    "    \n",
    "    return phase\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_BASE = 3e-3\n",
    "P1 = make_phase(P1_SMILES, 1, 3e-3/2, 500)\n",
    "P2 = make_phase(P2_SMILES, 2,3e-4, 100)\n",
    "P3 = make_phase(P3_SMILES, 3, 3e-4/2, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "xi, yi, xlen = P1.traindata[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('GCOc1cc(O)c2C(=O)O[C@@H](C)CCC[C@@H](O)[C@@H](O)[C@@H](O)C\\\\C=C\\\\c2c1\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA',\n",
       " 'COc1cc(O)c2C(=O)O[C@@H](C)CCC[C@@H](O)[C@@H](O)[C@@H](O)C\\\\C=C\\\\c2c1\\nAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode(xi), decode(yi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_valid(smiles):\n",
    "    \"\"\"\n",
    "    count_valid(smiles):\n",
    "        - returns % of valid smiles\n",
    "    \"\"\"\n",
    "    invalid = 0\n",
    "    \n",
    "    for m in map(Chem.MolFromSmiles, smiles):\n",
    "        invalid += m is None\n",
    "        \n",
    "    return 1 - (invalid/len(smiles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plain LSTM model\n",
    "class GeneratorLSTM(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_layers, hidden_size, embedding_size, bidirectional):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=embedding_size, \n",
    "                            hidden_size=hidden_size, \n",
    "                            num_layers=num_layers, \n",
    "                            batch_first=True,\n",
    "                            dropout=0.15,\n",
    "                            bidirectional=bidirectional)\n",
    "        \n",
    "        self.input_module = nn.Sequential(nn.Linear(input_size, 256),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(0.10),\n",
    "                                         nn.Linear(256, 512),\n",
    "                                         nn.ReLU(),\n",
    "                                         nn.Dropout(0.10),\n",
    "                                         nn.Linear(512, embedding_size),\n",
    "                                         nn.ReLU())\n",
    "        lstm_output_shape = hidden_size if not bidirectional else hidden_size * 2\n",
    "        self.output_module = nn.Sequential(nn.Linear(lstm_output_shape , 256),\n",
    "                                           nn.Dropout(0.10),\n",
    "                                           nn.ReLU(),\n",
    "                                           nn.Linear(256, output_size))\n",
    "\n",
    "        self.hidden = None \n",
    "\n",
    "    def init_hidden(self, batch_size, cuda=True):\n",
    "        if self.bidirectional:\n",
    "            ht = Variable(torch.zeros(self.num_layers*2, batch_size, self.hidden_size))\n",
    "            ct = Variable(torch.zeros(self.num_layers*2, batch_size, self.hidden_size))\n",
    "        else:\n",
    "            ht = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "            ct = Variable(torch.zeros(self.num_layers, batch_size, self.hidden_size))\n",
    "            \n",
    "\n",
    "        if cuda:\n",
    "            return ht.cuda(), ct.cuda()\n",
    "        else:\n",
    "            return ht, ct\n",
    "            \n",
    "\n",
    "\n",
    "    def forward(self, input_raw, pack=False, input_lens=None):\n",
    "        \"\"\"\n",
    "        forward(self, input_raw, state):\n",
    "             - input_raw = (bs, x_{i,t})\n",
    "             - state = (ht, ct)\n",
    "        \"\"\"\n",
    "        input_ = self.input_module(input_raw)\n",
    "        \n",
    "        if pack:\n",
    "            input_ = pack_padded_sequence(input_, input_lens, batch_first=True)\n",
    "            \n",
    "        input_, self.hidden = self.lstm(input_, self.hidden)\n",
    "        \n",
    "        if pack: \n",
    "            input_ = pad_packed_sequence(input_, batch_first=True)[0]\n",
    "\n",
    "            \n",
    "        input_ = self.output_module(input_)\n",
    "        return input_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = GeneratorLSTM(input_size = NUM_SYM, \n",
    "                      output_size = NUM_SYM, \n",
    "                      num_layers = 3, \n",
    "                      hidden_size = 512, \n",
    "                      embedding_size = 512,\n",
    "                      bidirectional=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorLSTM(\n",
       "  (lstm): LSTM(512, 512, num_layers=3, batch_first=True, dropout=0.15)\n",
       "  (input_module): Sequential(\n",
       "    (0): Linear(in_features=41, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (output_module): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=41, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GeneratorLSTM(\n",
       "  (lstm): LSTM(512, 512, num_layers=3, batch_first=True, dropout=0.15)\n",
       "  (input_module): Sequential(\n",
       "    (0): Linear(in_features=41, out_features=256, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Dropout(p=0.1, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=512, bias=True)\n",
       "    (4): ReLU()\n",
       "    (5): Dropout(p=0.1, inplace=False)\n",
       "    (6): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (7): ReLU()\n",
       "  )\n",
       "  (output_module): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): Dropout(p=0.1, inplace=False)\n",
       "    (2): ReLU()\n",
       "    (3): Linear(in_features=256, out_features=41, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "        \n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model has [6,850,601] trainable params\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'model has [{count_parameters(model):,}] trainable params')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, model_optim, lr_sched,\n",
    "                    train_dataloader, test_dataloader, \n",
    "                    test_interval, save_path, epoch):\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    loss_history = []\n",
    "    total = len(train_dataloader)\n",
    "    \n",
    "    test_loss_history = defaultdict(list)\n",
    "    new_smiles = defaultdict(list)\n",
    "    \n",
    "    \n",
    "    for batch_idx, batch in tqdm(enumerate(train_dataloader), total=total):\n",
    "        \n",
    "        if batch_idx % test_interval == 0:\n",
    "            new_smiles[batch_idx] = generate(model,)\n",
    "            \n",
    "            smiles_valid_batch = generate(model, test_samples=100, sample_f=softmax_temp_sample, v=False)\n",
    "            validity = count_valid(smiles_valid_batch)\n",
    "            \n",
    "            test_loss_history[batch_idx] = test(model, test_dataloader)\n",
    "            # we can test later\n",
    "            \n",
    "            print(f'[batch num: {batch_idx}] sampled; [{validity:.3f}%] valid smiles sampled')\n",
    "            \n",
    "            if batch_idx == 0:\n",
    "                print_loss = 'nan'\n",
    "            else:\n",
    "                print_loss = f'{loss.data.item():.4f}'\n",
    "\n",
    "            \n",
    "            path = save_path / Path(f'{epoch}-epoch-{batch_idx}-batch-{print_loss}-loss')\n",
    "            path.mkdir(exist_ok=True)\n",
    "            \n",
    "            torch.save(model.state_dict(), str(path / 'model_dict.torch'))\n",
    "            save(new_smiles[batch_idx], str(path / 'generated_smiles.list' ))\n",
    "            save(test_loss_history[batch_idx] , str(path / 'test_loss.item' ))\n",
    "        \n",
    "        model.train()\n",
    "        model.zero_grad()\n",
    "    \n",
    "        x_batch, y_batch, batch_lens = batch\n",
    "        \n",
    "        batch_size = x_batch.size(0)\n",
    "        max_len = int(max(batch_lens).item())\n",
    "        \n",
    "        x_batch = x_batch[:, 0:max_len, :]\n",
    "        y_batch = y_batch[:, 0:max_len, :]\n",
    "\n",
    "        # sort input\n",
    "        batch_len_sorted, sort_index = torch.sort(batch_lens, 0, descending=True)\n",
    "        batch_len_sorted = batch_len_sorted.numpy().tolist()\n",
    "        \n",
    "        x_batch = torch.index_select(x_batch, 0, sort_index)\n",
    "        y_batch = torch.index_select(y_batch, 0, sort_index)\n",
    "\n",
    "        x_batch = Variable(x_batch.float()).cuda()\n",
    "        y_batch = Variable(y_batch.float()).cuda()\n",
    "\n",
    "        # init state\n",
    "        model.hidden = model.init_hidden(batch_size=x_batch.size(0))\n",
    "        try:\n",
    "            y_pred = model(x_batch, pack=True, input_lens=batch_len_sorted)\n",
    "        except Exception as e:\n",
    "            print(f'[ERROR] got exception {e}')\n",
    "            print(f'[ERROR] skipping batch...')\n",
    "            continue\n",
    "        \n",
    "        y_pred = F.log_softmax(y_pred.view(-1, NUM_SYM), dim=-1)\n",
    "        _, y_batch = y_batch.topk(1, dim=-1)\n",
    "        y_batch = y_batch.view(-1)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "        lr_sched.step()\n",
    "\n",
    "        \n",
    "        if batch_idx % 10 == 0 :\n",
    "            print(f'[batch num: {batch_idx}] loss: {loss.data.item():.4f}')\n",
    "            \n",
    "    path = save_path / Path(f'{epoch}-epoch-END-batch')\n",
    "    path.mkdir(exist_ok=True)\n",
    "\n",
    "    torch.save(model.state_dict(), str(path / 'model_dict.torch'))\n",
    "\n",
    "                \n",
    "    return loss_history, test_loss_history, new_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_run_id():\n",
    "    return  np.random.choice(['cheetah', 'jaguar', 'wombat', 'cobra', \n",
    "                               'croc', 'panda', 'dragon', 'seal', 'spider', 'lizard',\n",
    "                               'gorilla', 'koala', 'blackbear', 'grizzly', 'zebra',\n",
    "                               'hippo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dragon'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_run_id()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer_learning(model, phases, epochs, batch_size=32, run_id = None):\n",
    "    \"\"\"\n",
    "    transfer_learning(model, phases):\n",
    "        - model: pytorch model\n",
    "        - phases: [Datasets] for datapaths\n",
    "    \"\"\"\n",
    "    history = defaultdict(dict)\n",
    "    \n",
    "    run_id = run_id if run_id is not None else make_run_id()\n",
    "    \n",
    "    results_dir = DATA_PATH / 'results' / f'{run_id}-run'\n",
    "    results_dir.mkdir(exist_ok=False)\n",
    "    save(SYM_TO_ID, results_dir / 'SYM_TO_ID')\n",
    "    save(ID_TO_SYM, results_dir / 'ID_TO_SYM')\n",
    "\n",
    "    print(f'[TRANSFER_LEARNER] saving all progress to [{results_dir}]')\n",
    "    \n",
    "    for idx, phase in enumerate(phases):\n",
    "        print(f'[TRANSFER_LEARNER] STARTING PHASE [{phase.pid}]')\n",
    "        \n",
    "        dataloader = DataLoader(phase.traindata, \n",
    "                                batch_size=batch_size, \n",
    "                                num_workers=NUM_CPU, \n",
    "                                shuffle=True)\n",
    "        \n",
    "        testloader = DataLoader(phase.testdata, \n",
    "                                batch_size=1000, \n",
    "                                num_workers=NUM_CPU, \n",
    "                                shuffle=True)\n",
    "        \n",
    "        model_optim = optim.Adam(list(model.parameters()), lr=phase.lr)\n",
    "        lr_sched = CosineAnnealingLR(model_optim, 500)\n",
    "        \n",
    "        phase_dir = results_dir / f'phase-{phase.pid}'\n",
    "        phase_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        for epoch_idx in range(epochs[idx]):\n",
    "            print(f'[TRANSFER_LEARNER] PHASE [{phase.pid}] EPOCH [{epoch_idx}]')\n",
    "            history[phase.pid][epoch_idx] = train_one_epoch(model, model_optim, lr_sched,\n",
    "                                                   dataloader, testloader,\n",
    "                                                   phase.test_interval, phase_dir, \n",
    "                                                   epoch_idx)\n",
    "            \n",
    "    return history\n",
    "            \n",
    "            \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing and Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_temp_sample(y_t, temperature = 1.0):\n",
    "    prediction_vector = F.softmax(y_t / temperature, dim=-1)\n",
    "    x_index_t = torch.multinomial(prediction_vector, 1)[:, 0]\n",
    "    return x_index_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_sample(y_t):\n",
    "    _, pred_idx =  y_t.topk(1, dim=-1)\n",
    "    return pred_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, test_samples=5, sample_f=softmax_temp_sample, v=True):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "\n",
    "        x = torch.zeros(test_samples, MAX_SYM, NUM_SYM).cuda()\n",
    "\n",
    "        x[:, 0, SYM_TO_ID[GO_TOKEN]] = 1\n",
    "        \n",
    "        for i in range(MAX_SYM-1):\n",
    "            model.hidden = model.init_hidden(batch_size=test_samples)\n",
    "            \n",
    "            pred = model(x, pack=True, input_lens=np.ones(test_samples)*(i+1))\n",
    "            pred_idx = sample_f(pred[:,i,:])\n",
    "            temp = torch.zeros(test_samples, MAX_SYM, NUM_SYM).cuda()\n",
    "            for j in range(test_samples):\n",
    "                temp[j, i+1, pred_idx[j]] = 1  \n",
    "            x.add_(temp)\n",
    "\n",
    "        if v: print('\\n',10*'-' + 'GENERATED SMILES STRINGS' + 10*'-')\n",
    "        smiles = []\n",
    "\n",
    "        for j in range(test_samples):\n",
    "            s = decode_valid(x[j].cpu().numpy())\n",
    "            smiles += [s]\n",
    "\n",
    "            if v: print(s)\n",
    "\n",
    "        return smiles\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, testloader):\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        criterion = nn.NLLLoss()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in tqdm(testloader,total=len(testloader)):\n",
    "\n",
    "            x_batch, y_batch, batch_lens = batch\n",
    "\n",
    "            batch_size = x_batch.size(0)\n",
    "            max_len = int(max(batch_lens).item())\n",
    "\n",
    "            x_batch = x_batch[:, 0:max_len, :]\n",
    "            y_batch = y_batch[:, 0:max_len, :]\n",
    "\n",
    "            # sort input\n",
    "            batch_len_sorted, sort_index = torch.sort(batch_lens, 0, descending=True)\n",
    "            batch_len_sorted = batch_len_sorted.numpy().tolist()\n",
    "\n",
    "            x_batch = torch.index_select(x_batch, 0, sort_index)\n",
    "            y_batch = torch.index_select(y_batch, 0, sort_index)\n",
    "\n",
    "            x_batch = Variable(x_batch.float()).cuda()\n",
    "            y_batch = Variable(y_batch.float()).cuda()\n",
    "\n",
    "            # init state\n",
    "            model.hidden = model.init_hidden(batch_size=x_batch.size(0))\n",
    "\n",
    "            y_pred = model(x_batch, pack=True, input_lens=batch_len_sorted)\n",
    "\n",
    "            y_pred = y_pred.view(-1, NUM_SYM)\n",
    "            _, y_batch = y_batch.topk(1, dim=-1)\n",
    "            y_batch = y_batch.view(-1)\n",
    "\n",
    "            total_loss += criterion(y_pred, y_batch)\n",
    "\n",
    "        return total_loss.data.item() / len(testloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved [<class 'dict'>] to data/lipinski/results/zebra-run/SYM_TO_ID\n",
      "saved [<class 'dict'>] to data/lipinski/results/zebra-run/ID_TO_SYM\n",
      "[TRANSFER_LEARNER] saving all progress to [data/lipinski/results/zebra-run]\n",
      "[TRANSFER_LEARNER] STARTING PHASE [1]\n",
      "[TRANSFER_LEARNER] PHASE [1] EPOCH [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5227c8a5e6a46a1b7934d5c948bde0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16544.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "\\FoHs9rC5[0o=op49%o7Oo2(=@rpC#P7=4cn5p+2pcG9GPN/pO1sG[Fl2rs\\/=ls(O01l]-1@F3I#\\B[[]\\8onOpoS4=0]3)GA@7[s[072@B=S6rppB1r-+7p03[SOo0HAsBF[\n",
      "5c66I4r5]noAA4lPGC[H]-2-7537=4-0\\n96OB%c)=1o@OG\\G@)s7PApG@C//75\\+sCFS\\l1=C9(4p5N-F8N%@3Ap(Sr5[)5#p@%p\n",
      "84o2O4B----6n]A)+#S\n",
      "7BPp=53rA[/46]8//Bp8//0(551#A/%S0]N4PG89\\\n",
      "8Ir33F)(G)76)8oBG)F\\2lFO/F+r\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7544b7f8961448b49d6bc4d16d089a15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 0] sampled; [0.050%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-0-batch-nan-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-0-batch-nan-loss/test_loss.item\n",
      "[batch num: 0] loss: 3.7025\n",
      "[batch num: 10] loss: 3.0124\n",
      "[batch num: 20] loss: 2.8118\n",
      "[batch num: 30] loss: 2.6824\n",
      "[batch num: 40] loss: 2.5676\n",
      "[batch num: 50] loss: 2.3619\n",
      "[batch num: 60] loss: 1.9394\n",
      "[batch num: 70] loss: 1.9654\n",
      "[batch num: 80] loss: 1.3544\n",
      "[batch num: 90] loss: 1.2640\n",
      "[batch num: 100] loss: 1.4756\n",
      "[batch num: 110] loss: 1.4155\n",
      "[batch num: 120] loss: 0.9922\n",
      "[batch num: 130] loss: 1.2545\n",
      "[batch num: 140] loss: 1.2949\n",
      "[batch num: 150] loss: 1.2264\n",
      "[batch num: 160] loss: 1.0949\n",
      "[batch num: 170] loss: 1.0276\n",
      "[batch num: 180] loss: 1.2213\n",
      "[batch num: 190] loss: 1.0423\n",
      "[batch num: 200] loss: 0.7628\n",
      "[batch num: 210] loss: 0.7687\n",
      "[batch num: 220] loss: 1.0216\n",
      "[batch num: 230] loss: 0.9241\n",
      "[batch num: 240] loss: 0.9558\n",
      "[batch num: 250] loss: 0.9542\n",
      "[batch num: 260] loss: 0.9274\n",
      "[batch num: 270] loss: 0.8865\n",
      "[batch num: 280] loss: 0.8568\n",
      "[batch num: 290] loss: 1.1152\n",
      "[batch num: 300] loss: 0.9432\n",
      "[batch num: 310] loss: 0.8193\n",
      "[batch num: 320] loss: 0.9827\n",
      "[batch num: 330] loss: 1.0248\n",
      "[batch num: 340] loss: 0.9463\n",
      "[batch num: 350] loss: 0.9927\n",
      "[batch num: 360] loss: 0.8205\n",
      "[batch num: 370] loss: 1.0117\n",
      "[batch num: 380] loss: 0.9320\n",
      "[batch num: 390] loss: 0.8508\n",
      "[batch num: 400] loss: 0.8010\n",
      "[batch num: 410] loss: 0.8629\n",
      "[batch num: 420] loss: 0.8555\n",
      "[batch num: 430] loss: 0.9520\n",
      "[batch num: 440] loss: 0.8886\n",
      "[batch num: 450] loss: 0.7924\n",
      "[batch num: 460] loss: 1.0192\n",
      "[batch num: 470] loss: 0.7925\n",
      "[batch num: 480] loss: 0.7971\n",
      "[batch num: 490] loss: 0.7149\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "OCOC(CC(C)C(=N)NC1CCCCC1NCC(=O)c2ccnc3ccccc4\n",
      "NC(=O)Nc1ccccc2)n1ncc(OC)c(SC(=O)NCC3=[O)c3ccccc3FC)c(OC)n1)O=C)(C)(C)F)NCN\n",
      "CSCC(CCN1(CC(=O)CCc2cc(cc(O)cc2)c2ccccc5)C(=O)C4\n",
      "COc1ccc2ccc(cc1C(=O)(=O)c2cccnc5)c31)cnc43C\n",
      "Clc1ccc(cc1)n2ccc(cc2)C(=O)N3C(C(N2CN(=O)N\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbd5a1e96b84030b55726200cfd4726",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 500] sampled; [0.000%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-500-batch-0.9426-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-500-batch-0.9426-loss/test_loss.item\n",
      "[batch num: 500] loss: 0.9236\n",
      "[batch num: 510] loss: 0.7213\n",
      "[batch num: 520] loss: 0.9427\n",
      "[batch num: 530] loss: 0.8451\n",
      "[batch num: 540] loss: 0.7914\n",
      "[batch num: 550] loss: 0.8685\n",
      "[batch num: 560] loss: 0.7428\n",
      "[batch num: 570] loss: 0.8778\n",
      "[batch num: 580] loss: 0.8734\n",
      "[batch num: 590] loss: 0.9460\n",
      "[batch num: 600] loss: 0.7148\n",
      "[batch num: 610] loss: 0.8984\n",
      "[batch num: 620] loss: 0.7510\n",
      "[batch num: 630] loss: 0.9479\n",
      "[batch num: 640] loss: 0.6416\n",
      "[batch num: 650] loss: 0.9046\n",
      "[batch num: 660] loss: 0.9202\n",
      "[batch num: 670] loss: 0.7619\n",
      "[batch num: 680] loss: 1.0050\n",
      "[batch num: 690] loss: 0.7953\n",
      "[batch num: 700] loss: 0.9173\n",
      "[batch num: 710] loss: 0.6275\n",
      "[batch num: 720] loss: 0.7771\n",
      "[batch num: 730] loss: 0.9545\n",
      "[batch num: 740] loss: 0.7035\n",
      "[batch num: 750] loss: 0.7831\n",
      "[batch num: 760] loss: 0.8844\n",
      "[batch num: 770] loss: 0.7188\n",
      "[batch num: 780] loss: 0.6383\n",
      "[batch num: 790] loss: 0.8863\n",
      "[batch num: 800] loss: 0.8047\n",
      "[batch num: 810] loss: 0.8321\n",
      "[batch num: 820] loss: 0.6728\n",
      "[batch num: 830] loss: 0.8112\n",
      "[batch num: 840] loss: 0.6924\n",
      "[batch num: 850] loss: 0.9149\n",
      "[batch num: 860] loss: 0.6047\n",
      "[batch num: 870] loss: 0.7702\n",
      "[batch num: 880] loss: 0.8561\n",
      "[batch num: 890] loss: 0.7579\n",
      "[batch num: 900] loss: 0.8369\n",
      "[batch num: 910] loss: 0.6537\n",
      "[batch num: 920] loss: 0.9925\n",
      "[batch num: 930] loss: 0.6847\n",
      "[batch num: 940] loss: 0.6741\n",
      "[batch num: 950] loss: 0.6519\n",
      "[batch num: 960] loss: 0.8208\n",
      "[batch num: 970] loss: 0.7112\n",
      "[batch num: 980] loss: 0.6521\n",
      "[batch num: 990] loss: 0.8444\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "COc1ccc2C[C@@]4(CC(=O)[C@@H](N)C(=O)N(C)Nc3ccc(Cl)cc2)C(=S)Nc3ccccc3)c4ccc[[nH]3)cc1\n",
      "COCCNC(=O)(=O)c1ccc2nc(Cc3ccccc24)\\N=C(F)c3ccc(OC)cc3)cc1\n",
      "Cc1nc(NC(=O)C(C)c2c(CN2CCC(CCC4N)c(OS(=O)c4ccc(F)cc3)cc13\n",
      "COC(=O)c1cc(NC(=O)c2ccc(NC(=O)Nc3ccc(sc2)C(F)(F)F)ccc1NC4=C(C)Cc5cncnn1)cc1\n",
      "Fc1cc2nc(N)c(CCNC(=O)C2)nn1c2[nH]c2c2COCc5ncn(nc3c4ccccc4)cc2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c085b81525a4e85804b37b1cceec320",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 1000] sampled; [0.010%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-1000-batch-0.8034-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-1000-batch-0.8034-loss/test_loss.item\n",
      "[batch num: 1000] loss: 0.8474\n",
      "[batch num: 1010] loss: 0.7972\n",
      "[batch num: 1020] loss: 0.7590\n",
      "[batch num: 1030] loss: 0.7705\n",
      "[batch num: 1040] loss: 0.6459\n",
      "[batch num: 1050] loss: 0.8453\n",
      "[batch num: 1060] loss: 0.8374\n",
      "[batch num: 1070] loss: 0.6984\n",
      "[batch num: 1080] loss: 0.6287\n",
      "[batch num: 1090] loss: 0.7114\n",
      "[batch num: 1100] loss: 0.7659\n",
      "[batch num: 1110] loss: 0.6996\n",
      "[batch num: 1120] loss: 0.7093\n",
      "[batch num: 1130] loss: 0.7525\n",
      "[batch num: 1140] loss: 0.7357\n",
      "[batch num: 1150] loss: 0.8469\n",
      "[batch num: 1160] loss: 0.6841\n",
      "[batch num: 1170] loss: 0.7275\n",
      "[batch num: 1180] loss: 0.5487\n",
      "[batch num: 1190] loss: 0.6366\n",
      "[batch num: 1200] loss: 0.5407\n",
      "[batch num: 1210] loss: 0.4991\n",
      "[batch num: 1220] loss: 0.5412\n",
      "[batch num: 1230] loss: 0.5641\n",
      "[batch num: 1240] loss: 0.6645\n",
      "[batch num: 1250] loss: 0.7273\n",
      "[batch num: 1260] loss: 0.7004\n",
      "[batch num: 1270] loss: 0.5091\n",
      "[batch num: 1280] loss: 0.6266\n",
      "[batch num: 1290] loss: 0.4798\n",
      "[batch num: 1300] loss: 0.7626\n",
      "[batch num: 1310] loss: 0.5626\n",
      "[batch num: 1320] loss: 0.6249\n",
      "[batch num: 1330] loss: 0.6265\n",
      "[batch num: 1340] loss: 0.6268\n",
      "[batch num: 1350] loss: 0.5821\n",
      "[batch num: 1360] loss: 0.4385\n",
      "[batch num: 1370] loss: 0.6885\n",
      "[batch num: 1380] loss: 0.6818\n",
      "[batch num: 1390] loss: 0.6626\n",
      "[batch num: 1400] loss: 0.5253\n",
      "[batch num: 1410] loss: 0.6930\n",
      "[batch num: 1420] loss: 0.6139\n",
      "[batch num: 1430] loss: 0.6369\n",
      "[batch num: 1440] loss: 0.6623\n",
      "[batch num: 1450] loss: 0.5133\n",
      "[batch num: 1460] loss: 0.5303\n",
      "[batch num: 1470] loss: 0.4306\n",
      "[batch num: 1480] loss: 0.6327\n",
      "[batch num: 1490] loss: 0.5951\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "COc1cc(OC)c(CN2CCCC(CC2)N3Sc3ccc(N)cc3OS(=O)(=O)c2c1\n",
      "Cc1cccc(Br)c1OCC2CCC(=O)NC(C)C(F)(F)F)C(F)(F)F\n",
      "COCCCCN1C(=O)c2cc3ccccc2c2c1nc1NCCCN4CCOCC4\n",
      "O=C1N=C(Cn12)C(=N)C=CC2c3cccc(OCC#O)c3)cc1C\n",
      "NCCSCC(NC(=O)Nc1ncccc1F)c2ccc(OCC(C)(C)C)c3C(=O)c4ccsc4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdef69d0d4f04af4b50d6ef1aa00cd51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 1500] sampled; [0.080%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-1500-batch-0.5716-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-1500-batch-0.5716-loss/test_loss.item\n",
      "[batch num: 1500] loss: 0.6623\n",
      "[batch num: 1510] loss: 0.5997\n",
      "[batch num: 1520] loss: 0.4185\n",
      "[batch num: 1530] loss: 0.6382\n",
      "[batch num: 1540] loss: 0.6415\n",
      "[batch num: 1550] loss: 0.4935\n",
      "[batch num: 1560] loss: 0.6518\n",
      "[batch num: 1570] loss: 0.5776\n",
      "[batch num: 1580] loss: 0.6263\n",
      "[batch num: 1590] loss: 0.6716\n",
      "[batch num: 1600] loss: 0.5616\n",
      "[batch num: 1610] loss: 0.4897\n",
      "[batch num: 1620] loss: 0.6189\n",
      "[batch num: 1630] loss: 0.6226\n",
      "[batch num: 1640] loss: 0.5493\n",
      "[batch num: 1650] loss: 0.5888\n",
      "[batch num: 1660] loss: 0.6481\n",
      "[batch num: 1670] loss: 0.5458\n",
      "[batch num: 1680] loss: 0.5704\n",
      "[batch num: 1690] loss: 0.6662\n",
      "[batch num: 1700] loss: 0.6174\n",
      "[batch num: 1710] loss: 0.6706\n",
      "[batch num: 1720] loss: 0.6772\n",
      "[batch num: 1730] loss: 0.6066\n",
      "[batch num: 1740] loss: 0.6783\n",
      "[batch num: 1750] loss: 0.5241\n",
      "[batch num: 1760] loss: 0.5708\n",
      "[batch num: 1770] loss: 0.6115\n",
      "[batch num: 1780] loss: 0.6733\n",
      "[batch num: 1790] loss: 0.6657\n",
      "[batch num: 1800] loss: 0.6400\n",
      "[batch num: 1810] loss: 0.6385\n",
      "[batch num: 1820] loss: 0.5834\n",
      "[batch num: 1830] loss: 0.5476\n",
      "[batch num: 1840] loss: 0.5696\n",
      "[batch num: 1850] loss: 0.5440\n",
      "[batch num: 1860] loss: 0.5871\n",
      "[batch num: 1870] loss: 0.6445\n",
      "[batch num: 1880] loss: 0.6481\n",
      "[batch num: 1890] loss: 0.7188\n",
      "[batch num: 1900] loss: 0.6661\n",
      "[batch num: 1910] loss: 0.6221\n",
      "[batch num: 1920] loss: 0.5275\n",
      "[batch num: 1930] loss: 0.4968\n",
      "[batch num: 1940] loss: 0.7081\n",
      "[batch num: 1950] loss: 0.5209\n",
      "[batch num: 1960] loss: 0.5797\n",
      "[batch num: 1970] loss: 0.6508\n",
      "[batch num: 1980] loss: 0.5462\n",
      "[batch num: 1990] loss: 0.6384\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "COc1cc(\\C=C\\2\\c2nc3CCN(CCc4ccc4nccs4)cc12)n5nccn5\n",
      "COC1=O)N[C@@H](Oc2cc(C)c3cc(Cl)cc(c3C2=O)c4ccccc4N4CCNCC5\n",
      "CN(CCC)C(=O)c1oc2C(CC3C(=O)Cc3sc2c(cnc13)CC2\n",
      "CN(C)C(=O)c1cccc(OCCCN2CN(CCC2)C2=O)c3oc4CCC(CC(C5CC6)CO5)Cc5ccncc6\n",
      "COc1ccc(cc1)C(=O)NC2CC(CN2C(=O)c3cccn3)C(=O)NCCO\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fd26c219256492a9f768d3f91fbee08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 2000] sampled; [0.130%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-2000-batch-0.6841-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-2000-batch-0.6841-loss/test_loss.item\n",
      "[batch num: 2000] loss: 0.5795\n",
      "[batch num: 2010] loss: 0.6549\n",
      "[batch num: 2020] loss: 0.5868\n",
      "[batch num: 2030] loss: 0.4524\n",
      "[batch num: 2040] loss: 0.6880\n",
      "[batch num: 2050] loss: 0.6145\n",
      "\n",
      "[batch num: 2500] sampled; [0.260%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-2500-batch-0.4767-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-2500-batch-0.4767-loss/test_loss.item\n",
      "[batch num: 2500] loss: 0.5374\n",
      "[batch num: 2510] loss: 0.5921\n",
      "[batch num: 2520] loss: 0.5049\n",
      "[batch num: 2530] loss: 0.4835\n",
      "[batch num: 2540] loss: 0.4902\n",
      "[batch num: 2550] loss: 0.4847\n",
      "[batch num: 2560] loss: 0.5465\n",
      "[batch num: 2570] loss: 0.5371\n",
      "[batch num: 2580] loss: 0.4860\n",
      "[batch num: 2590] loss: 0.5328\n",
      "[batch num: 2600] loss: 0.6453\n",
      "[batch num: 2610] loss: 0.5735\n",
      "[batch num: 2620] loss: 0.6104\n",
      "[batch num: 2630] loss: 0.5766\n",
      "[batch num: 2640] loss: 0.5143\n",
      "[batch num: 2650] loss: 0.4354\n",
      "[batch num: 2660] loss: 0.5652\n",
      "[batch num: 2670] loss: 0.4726\n",
      "[batch num: 2680] loss: 0.6519\n",
      "[batch num: 2690] loss: 0.6055\n",
      "[batch num: 2700] loss: 0.5028\n",
      "[batch num: 2710] loss: 0.3686\n",
      "[batch num: 2720] loss: 0.5117\n",
      "[batch num: 2730] loss: 0.6320\n",
      "[batch num: 2740] loss: 0.5802\n",
      "[batch num: 2750] loss: 0.6003\n",
      "[batch num: 2760] loss: 0.4957\n",
      "[batch num: 2770] loss: 0.5744\n",
      "[batch num: 2780] loss: 0.5804\n",
      "[batch num: 2790] loss: 0.5428\n",
      "[batch num: 2800] loss: 0.6114\n",
      "[batch num: 2810] loss: 0.5808\n",
      "[batch num: 2820] loss: 0.5156\n",
      "[batch num: 2830] loss: 0.5078\n",
      "[batch num: 2840] loss: 0.6467\n",
      "[batch num: 2850] loss: 0.3908\n",
      "[batch num: 2860] loss: 0.5197\n",
      "[batch num: 2870] loss: 0.5311\n",
      "[batch num: 2880] loss: 0.5596\n",
      "[batch num: 2890] loss: 0.5448\n",
      "[batch num: 2900] loss: 0.5397\n",
      "[batch num: 2910] loss: 0.5215\n",
      "[batch num: 2920] loss: 0.5287\n",
      "[batch num: 2930] loss: 0.4929\n",
      "[batch num: 2940] loss: 0.4955\n",
      "[batch num: 2950] loss: 0.5428\n",
      "[batch num: 2960] loss: 0.4671\n",
      "[batch num: 2970] loss: 0.6112\n",
      "[batch num: 2980] loss: 0.5968\n",
      "[batch num: 2990] loss: 0.4656\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "COc1ccc(cc1OC)C2C(CC2)NC(=O)CSCC3=CC(=O)NC3=O\n",
      "Clc1nccc2c(Nc3ncccc3C(=S)Nc4ccc(Br)cc4)cc12\n",
      "COc1ccc(CC[C@H]2COC(=O)C3C[C@H]4Cc6cc(ccc15)C(=O)N(CC)O)n5ccccc5Cl\n",
      "CCOc1ccc(cn1)C(=O)NN2C(=O)N(C(C3=O)c3cncn3C2)c4ccc(OC)cc4\n",
      "O=C1CCS(=O)(=O)NC2CCC[C@@H](N)[C@@H]3C[C@H]3COC4c2c1OCc4ccc(OC)c(OC)c4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b49b1ab9c74b0aa0d21a96270f62cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 3000] sampled; [0.310%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-3000-batch-0.4448-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-3000-batch-0.4448-loss/test_loss.item\n",
      "[batch num: 3000] loss: 0.6637\n",
      "[batch num: 3010] loss: 0.4952\n",
      "[batch num: 3020] loss: 0.5398\n",
      "[batch num: 3030] loss: 0.5140\n",
      "[batch num: 3040] loss: 0.5381\n",
      "[batch num: 3050] loss: 0.6028\n",
      "[batch num: 3060] loss: 0.3804\n",
      "[batch num: 3070] loss: 0.5301\n",
      "[batch num: 3080] loss: 0.4654\n",
      "[batch num: 3090] loss: 0.4530\n",
      "[batch num: 3100] loss: 0.5403\n",
      "[batch num: 3110] loss: 0.6468\n",
      "[batch num: 3120] loss: 0.5510\n",
      "[batch num: 3130] loss: 0.5131\n",
      "[batch num: 3140] loss: 0.4732\n",
      "[batch num: 3150] loss: 0.5384\n",
      "[batch num: 3160] loss: 0.3777\n",
      "[batch num: 3170] loss: 0.5057\n",
      "[batch num: 3180] loss: 0.6362\n",
      "[batch num: 3190] loss: 0.5927\n",
      "[batch num: 3200] loss: 0.5909\n",
      "[batch num: 3210] loss: 0.5481\n",
      "[batch num: 3220] loss: 0.5076\n",
      "[batch num: 3230] loss: 0.3972\n",
      "[batch num: 3240] loss: 0.6175\n",
      "[batch num: 3250] loss: 0.5159\n",
      "[batch num: 3260] loss: 0.4410\n",
      "[batch num: 3270] loss: 0.5551\n",
      "[batch num: 3280] loss: 0.5095\n",
      "[batch num: 3290] loss: 0.5793\n",
      "[batch num: 3300] loss: 0.5033\n",
      "[batch num: 3310] loss: 0.4701\n",
      "[batch num: 3320] loss: 0.5836\n",
      "[batch num: 3330] loss: 0.5246\n",
      "[batch num: 3340] loss: 0.5090\n",
      "[batch num: 3350] loss: 0.5371\n",
      "[batch num: 3360] loss: 0.5571\n",
      "[batch num: 3370] loss: 0.4998\n",
      "[batch num: 3380] loss: 0.5416\n",
      "[batch num: 3390] loss: 0.5008\n",
      "[batch num: 3400] loss: 0.5498\n",
      "[batch num: 3410] loss: 0.6068\n",
      "[batch num: 3420] loss: 0.4995\n",
      "[batch num: 3430] loss: 0.4666\n",
      "[batch num: 3440] loss: 0.5878\n",
      "[batch num: 3450] loss: 0.3985\n",
      "[batch num: 3460] loss: 0.5280\n",
      "[batch num: 3470] loss: 0.5388\n",
      "[batch num: 3480] loss: 0.4498\n",
      "[batch num: 3490] loss: 0.4886\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "CCNC1=CN(CCc2c3CCCOc3cc2C1=O)c4ccc(F)cc4\n",
      "CCC(NC(=O)CN1C=C(C(=O)NC)[C@H]2CO[C@@H]([C@H]2C(=O)O)[C@H](c3ccccc3)c4ccccc4)C1=O\n",
      "CC(C)CCN1C=C(C#C(OCc2ccc(cc2)[N+](=O)[O-])C(C)C(=O)NC1)CCC(C)(C)C\n",
      "Cc1c(c2ccc(F)c(NC(=O)COCc3cc(OC)c(OC)c(OC)c3)c4ccccc24)cc1\n",
      "Cc1nc(cc(n1)N3CCN(CC2)C(=O)Cn3ccnc3)C(=O)Cc4cccc(Cl)c4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "887332b9fcc64fafa7fb50a80982ad70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 3500] sampled; [0.420%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-3500-batch-0.4965-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-3500-batch-0.4965-loss/test_loss.item\n",
      "[batch num: 3500] loss: 0.3804\n",
      "[batch num: 3510] loss: 0.4142\n",
      "[batch num: 3520] loss: 0.6229\n",
      "[batch num: 3530] loss: 0.4767\n",
      "[batch num: 3540] loss: 0.5589\n",
      "[batch num: 3550] loss: 0.4675\n",
      "[batch num: 3560] loss: 0.5100\n",
      "[batch num: 3570] loss: 0.5733\n",
      "[batch num: 3580] loss: 0.4524\n",
      "[batch num: 3590] loss: 0.4806\n",
      "[batch num: 3600] loss: 0.5092\n",
      "[batch num: 3610] loss: 0.5473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 5500] sampled; [0.590%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-5500-batch-0.4805-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-5500-batch-0.4805-loss/test_loss.item\n",
      "[batch num: 5500] loss: 0.3295\n",
      "[batch num: 5510] loss: 0.4641\n",
      "[batch num: 5520] loss: 0.4435\n",
      "[batch num: 5530] loss: 0.4942\n",
      "[batch num: 5540] loss: 0.5344\n",
      "[batch num: 5550] loss: 0.6026\n",
      "[batch num: 5560] loss: 0.4049\n",
      "[batch num: 5570] loss: 0.5411\n",
      "[batch num: 5580] loss: 0.4610\n",
      "[batch num: 5590] loss: 0.5416\n",
      "[batch num: 5600] loss: 0.4629\n",
      "[batch num: 5610] loss: 0.5062\n",
      "[batch num: 5620] loss: 0.3946\n",
      "[batch num: 5630] loss: 0.5387\n",
      "[batch num: 5640] loss: 0.5485\n",
      "[batch num: 5650] loss: 0.4739\n",
      "[batch num: 5660] loss: 0.5222\n",
      "[batch num: 5670] loss: 0.5441\n",
      "[batch num: 5680] loss: 0.4798\n",
      "[batch num: 5690] loss: 0.5574\n",
      "[batch num: 5700] loss: 0.4938\n",
      "[batch num: 5710] loss: 0.4107\n",
      "[batch num: 5720] loss: 0.4378\n",
      "[batch num: 5730] loss: 0.4948\n",
      "[batch num: 5740] loss: 0.4929\n",
      "[batch num: 5750] loss: 0.5094\n",
      "[batch num: 5760] loss: 0.3652\n",
      "[batch num: 5770] loss: 0.4855\n",
      "[batch num: 5780] loss: 0.5643\n",
      "[batch num: 5790] loss: 0.4374\n",
      "[batch num: 5800] loss: 0.4972\n",
      "[batch num: 5810] loss: 0.5438\n",
      "[batch num: 5820] loss: 0.4992\n",
      "[batch num: 5830] loss: 0.5624\n",
      "[batch num: 5840] loss: 0.4048\n",
      "[batch num: 5850] loss: 0.4897\n",
      "[batch num: 5860] loss: 0.3875\n",
      "[batch num: 5870] loss: 0.4813\n",
      "[batch num: 5880] loss: 0.5679\n",
      "[batch num: 5890] loss: 0.4692\n",
      "[batch num: 5900] loss: 0.5055\n",
      "[batch num: 5910] loss: 0.5113\n",
      "[batch num: 5920] loss: 0.5130\n",
      "[batch num: 5930] loss: 0.5215\n",
      "[batch num: 5940] loss: 0.5596\n",
      "[batch num: 5950] loss: 0.5267\n",
      "[batch num: 5960] loss: 0.4854\n",
      "[batch num: 5970] loss: 0.4982\n",
      "[batch num: 5980] loss: 0.3693\n",
      "[batch num: 5990] loss: 0.5558\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "CCCC1(CC1)Oc2nnc(SCC(=O)c3ccc4OCOc4c3)n2CCC(=O)O\n",
      "COC(=O)c1nccs1C(=O)Nc2ccc(cc2)C(=O)NC3CC3\n",
      "CN(C)CC(=O)OCC1(CC1)C(=O)OCc2ccccc2\n",
      "Clc1ccc2c(c1)nc(SCC(=O)Nc3ccc(cc3F)c4nnc(NC5CC4)c5)c2CC5CC4\n",
      "CN(C)[C@]1(CCCN(C1)c2ncnc3ccc(OCc4oc5ccccc5c4)ccc23)C(=O)O\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7fdf6e249c3f4e8497cab47964237fa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 6000] sampled; [0.590%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-6000-batch-0.4984-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-6000-batch-0.4984-loss/test_loss.item\n",
      "[batch num: 6000] loss: 0.5503\n",
      "[batch num: 6010] loss: 0.4409\n",
      "[batch num: 6020] loss: 0.4949\n",
      "[batch num: 6030] loss: 0.3744\n",
      "[batch num: 6040] loss: 0.3784\n",
      "[batch num: 6050] loss: 0.5230\n",
      "[batch num: 6060] loss: 0.5164\n",
      "[batch num: 6070] loss: 0.4509\n",
      "[batch num: 6080] loss: 0.4778\n",
      "[batch num: 6090] loss: 0.4912\n",
      "[batch num: 6100] loss: 0.5798\n",
      "[batch num: 6110] loss: 0.3188\n",
      "[batch num: 6120] loss: 0.4531\n",
      "[batch num: 6130] loss: 0.4343\n",
      "[batch num: 6140] loss: 0.4416\n",
      "[batch num: 6150] loss: 0.4722\n",
      "[batch num: 6160] loss: 0.5055\n",
      "[batch num: 6170] loss: 0.5568\n",
      "[batch num: 6180] loss: 0.4600\n",
      "[batch num: 6190] loss: 0.3782\n",
      "[batch num: 6200] loss: 0.4783\n",
      "[batch num: 6210] loss: 0.4660\n",
      "[batch num: 6220] loss: 0.3963\n",
      "[batch num: 6230] loss: 0.4175\n",
      "[batch num: 6240] loss: 0.5149\n",
      "[batch num: 6250] loss: 0.4259\n",
      "[batch num: 6260] loss: 0.4665\n",
      "[batch num: 6270] loss: 0.4576\n",
      "[batch num: 6280] loss: 0.4759\n",
      "[batch num: 6290] loss: 0.5613\n",
      "[batch num: 6300] loss: 0.3854\n",
      "[batch num: 6310] loss: 0.5054\n",
      "[batch num: 6320] loss: 0.4907\n",
      "[batch num: 6330] loss: 0.5277\n",
      "[batch num: 6340] loss: 0.4457\n",
      "[batch num: 6350] loss: 0.4502\n",
      "[batch num: 6360] loss: 0.5320\n",
      "[batch num: 6370] loss: 0.5472\n",
      "[batch num: 6380] loss: 0.4882\n",
      "[batch num: 6390] loss: 0.4239\n",
      "[batch num: 6400] loss: 0.4429\n",
      "[batch num: 6410] loss: 0.4946\n",
      "[batch num: 6420] loss: 0.4987\n",
      "[batch num: 6430] loss: 0.4332\n",
      "[batch num: 6440] loss: 0.3841\n",
      "[batch num: 6450] loss: 0.5632\n",
      "[batch num: 6460] loss: 0.4501\n",
      "[batch num: 6470] loss: 0.4178\n",
      "[batch num: 6480] loss: 0.5112\n",
      "[batch num: 6490] loss: 0.3602\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "CN(C1CCOC1)S(=O)(=O)c2ccc(cc2)S(=O)(=O)Nc3nn[nH]n3\n",
      "Cc1ccccc1NS(=O)(=O)c2ccc(c(c2)C(F)(F)F)C(=O)NCc3ccccc3\n",
      "O=C(COc1ccc2OCOc2c1)S(=O)(=O)N3CCc4c(C2)cccn4\n",
      "Nc1nc(CNC(=O)c2cccc(c2)C(F)(F)F)c(c3cc(on3)C#N)cc1F\n",
      "COc1ccccc1NC(=O)CN2C(=O)c3cc(ccc3C2=O)c4ccccc4Br\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "563ed2c7503e4a49a33c8cea2bdcfad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 6500] sampled; [0.600%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-6500-batch-0.4577-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-6500-batch-0.4577-loss/test_loss.item\n",
      "[batch num: 6500] loss: 0.5199\n",
      "[batch num: 6510] loss: 0.4046\n",
      "[batch num: 6520] loss: 0.3975\n",
      "[batch num: 6530] loss: 0.5062\n",
      "[batch num: 6540] loss: 0.5050\n",
      "[batch num: 6550] loss: 0.4368\n",
      "[batch num: 6560] loss: 0.3573\n",
      "[batch num: 6570] loss: 0.3663\n",
      "[batch num: 6580] loss: 0.3607\n",
      "[batch num: 6590] loss: 0.4704\n",
      "[batch num: 6600] loss: 0.5523\n",
      "[batch num: 6610] loss: 0.4669\n",
      "[batch num: 6620] loss: 0.3023\n",
      "[batch num: 6630] loss: 0.4851\n",
      "[batch num: 6640] loss: 0.4309\n",
      "[batch num: 6650] loss: 0.4222\n",
      "[batch num: 6660] loss: 0.5142\n",
      "[batch num: 6670] loss: 0.4086\n",
      "[batch num: 6680] loss: 0.4643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[batch num: 8440] loss: 0.4876\n",
      "[batch num: 8450] loss: 0.4251\n",
      "[batch num: 8460] loss: 0.3993\n",
      "[batch num: 8470] loss: 0.4616\n",
      "[batch num: 8480] loss: 0.4740\n",
      "[batch num: 8490] loss: 0.4419\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "Cc1nnc[NS(=O)(=O)c2cc(F)cc(c2)c3ccc(cc3)C(F)(F)F)n1C(C)C4CCNC4=O\n",
      "CCCCN1C(=O)NC(=O)c2cc(NC=N)c(C(=O)c3ccc(Cl)cc3)cc12\n",
      "C[C@H]1COCCN1S(=O)(=O)c2nnc(NC(=O)c3cc4CC(C)CCn4n3)n2\n",
      "C[C@H]1CC[C@H]2OC(=O)[C@@]34CCCC[C@]5(C)[C@]3(CCN3CCC4)CC[C@H]4CC[C@]2(C)C\n",
      "Nc1cnc(nc1NC2CC(=O)NC2)c3nc4ccc(COc5ccc(F)cc5F)c4C(=O)c34\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05b0c0fdf5e049949aba57a71f002136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 8500] sampled; [0.730%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-8500-batch-0.4463-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-8500-batch-0.4463-loss/test_loss.item\n",
      "[batch num: 8500] loss: 0.4904\n",
      "[batch num: 8510] loss: 0.5157\n",
      "[batch num: 8520] loss: 0.4327\n",
      "[batch num: 8530] loss: 0.3678\n",
      "[batch num: 8540] loss: 0.4531\n",
      "[batch num: 8550] loss: 0.4099\n",
      "[batch num: 8560] loss: 0.3441\n",
      "[batch num: 8570] loss: 0.4668\n",
      "[batch num: 8580] loss: 0.4635\n",
      "[batch num: 8590] loss: 0.4113\n",
      "[batch num: 8600] loss: 0.4396\n",
      "[batch num: 8610] loss: 0.3923\n",
      "[batch num: 8620] loss: 0.4453\n",
      "[batch num: 8630] loss: 0.4750\n",
      "[batch num: 8640] loss: 0.3430\n",
      "[batch num: 8650] loss: 0.4433\n",
      "[batch num: 8660] loss: 0.4160\n",
      "[batch num: 8670] loss: 0.3586\n",
      "[batch num: 8680] loss: 0.5194\n",
      "[batch num: 8690] loss: 0.4560\n",
      "[batch num: 8700] loss: 0.4477\n",
      "[batch num: 8710] loss: 0.4249\n",
      "[batch num: 8720] loss: 0.3320\n",
      "[batch num: 8730] loss: 0.4848\n",
      "[batch num: 8740] loss: 0.4169\n",
      "[batch num: 8750] loss: 0.5179\n",
      "[batch num: 8760] loss: 0.4750\n",
      "[batch num: 8770] loss: 0.3424\n",
      "[batch num: 8780] loss: 0.4067\n",
      "[batch num: 8790] loss: 0.4934\n",
      "[batch num: 8800] loss: 0.4515\n",
      "[batch num: 8810] loss: 0.5860\n",
      "[batch num: 8820] loss: 0.4841\n",
      "[batch num: 8830] loss: 0.5148\n",
      "[batch num: 8840] loss: 0.4975\n",
      "[batch num: 8850] loss: 0.4495\n",
      "[batch num: 8860] loss: 0.4389\n",
      "[batch num: 8870] loss: 0.4985\n",
      "[batch num: 8880] loss: 0.4118\n",
      "[batch num: 8890] loss: 0.3971\n",
      "[batch num: 8900] loss: 0.3720\n",
      "[batch num: 8910] loss: 0.4860\n",
      "[batch num: 8920] loss: 0.3446\n",
      "[batch num: 8930] loss: 0.5595\n",
      "[batch num: 8940] loss: 0.4822\n",
      "[batch num: 8950] loss: 0.5505\n",
      "[batch num: 8960] loss: 0.4058\n",
      "[batch num: 8970] loss: 0.5026\n",
      "[batch num: 8980] loss: 0.4249\n",
      "[batch num: 8990] loss: 0.5160\n",
      "\n",
      " ----------GENERATED SMILES STRINGS----------\n",
      "CNC(=O)c1c(C(=O)N1CCCCCCCCC(=O)OC)c2ccc(C)cc2\n",
      "COc1cc2[nH]c3nc(ccc3c(cc2c1)N(CCc4ccccc4)C(=O)N)C3=O\n",
      "Cn1cnc(NC(=O)c2cccc3ccccc23)n1CCCc4ccccc4\n",
      "COc1ccc(cc1OC)[C@@H](NC(=O)c2cccc[n+]2[O-])C3CCCC3\n",
      "COc1cc(cc(c1)[N+](=O)[O-])C(=O)N[C@@H](Cc2ccccc2)C(=O)NC(Cc3ccccc3)C(=O)OCC=C\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b899af265c2a42fd9a98df631d0700bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=94.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[batch num: 9000] sampled; [0.650%] valid smiles sampled\n",
      "saved [<class 'list'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-9000-batch-0.4953-loss/generated_smiles.list\n",
      "saved [<class 'float'>] to data/lipinski/results/zebra-run/phase-1/0-epoch-9000-batch-0.4953-loss/test_loss.item\n",
      "[batch num: 9000] loss: 0.4453\n",
      "[batch num: 9010] loss: 0.4814\n",
      "[batch num: 9020] loss: 0.4705\n",
      "[batch num: 9030] loss: 0.4758\n",
      "[batch num: 9040] loss: 0.4147\n",
      "[batch num: 9050] loss: 0.4416\n",
      "[batch num: 9060] loss: 0.4985\n",
      "[batch num: 9070] loss: 0.5556\n",
      "[batch num: 9080] loss: 0.4795\n",
      "[batch num: 9090] loss: 0.4606\n",
      "[batch num: 9100] loss: 0.4838\n",
      "[batch num: 9110] loss: 0.4323\n",
      "[batch num: 9120] loss: 0.4906\n",
      "[batch num: 9130] loss: 0.4657\n",
      "[batch num: 9140] loss: 0.4973\n",
      "[batch num: 9150] loss: 0.4578\n",
      "[batch num: 9160] loss: 0.3402\n"
     ]
    }
   ],
   "source": [
    "history = transfer_learning(model, phases=[P1, P2, P3], epochs=[2, 2, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history, l=-1):\n",
    "    if l < 0:\n",
    "        l = len(history)\n",
    "    fig = px.line(x=np.arange(l), y=history[:l], labels={'x':'batch number', 'y':'binary cross-entropy loss'})\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls data/lipinski/results/panda-run/phase-0/0-epoch-3500-batch-0.4833-loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load('data/lipinski/results/panda-run/phase-0/0-epoch-3500-batch-0.4833-loss/generated_smiles.dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_path = './data/lipinski/results/panda-run/phase-0/0-epoch-3500-batch-0.4833-loss/model_dict.torch'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsmiles = generate(model, test_samples=500, sample_f=softmax_temp_sample)\n",
    "validity = count_valid(newsmiles)\n",
    "print(f'validity is at [{validity:.2f}%]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_learning(model, [P2, P3], [2, 2] )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
